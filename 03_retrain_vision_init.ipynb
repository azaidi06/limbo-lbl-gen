{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa561c4-0337-47dd-9bcd-d01dc5e7b230",
   "metadata": {},
   "source": [
    "Lets extract our VISION model and parse out how we can begin trying to retrain the weights for this -- we have some label information for our images (container present or not) which we can probably use to fine tune our vision model to better understand the type of pictures that will be thrown at it - for this application.\n",
    "\n",
    "*effectively we're trying to overfit the vision model on the images that we eventually want to label*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48156f34-3c82-4fea-bd5c-7cc3fe01b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51acb3ae-02d0-4f3d-b216-b1543fb502cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, device='cuda'):\n",
    "    processor = Blip2Processor.from_pretrained(model_name)\n",
    "    model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "                            model_name, \n",
    "                            ).to(device)\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def get_file_names(path, extension='.jpg'): \n",
    "    return [file_name for file_name in os.listdir(path) if file_name[-4:] == '.png']\n",
    "\n",
    "\n",
    "def get_img_paths(folder_path, img_names):\n",
    "     return [str(folder_path) + '/' + img_name for img_name in img_names]\n",
    "    \n",
    "\n",
    "def get_imgs(img_paths): return [Image.open(path) for path in img_paths]\n",
    "\n",
    "\n",
    "def get_lbls(images, processor, model, device=None):\n",
    "    if device is None: device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    inputs = processor(images=images, return_tensors='pt').to(device, torch.float32)\n",
    "    generated_ids = model.generate(**inputs)\n",
    "    return processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "def get_batch_idxs(img_paths, batch_size=10):\n",
    "    idxs = [x for x in range(0, len(img_paths)+1, batch_size)]\n",
    "    if len(img_paths) % batch_size != 0:\n",
    "        last_idx = len(img_paths) - 1\n",
    "        idxs.append(last_idx + 1)\n",
    "    idx_tuples = [(idxs[x-1], idxs[x]) for x in range(1, len(idxs))]\n",
    "    return idx_tuples\n",
    "\n",
    "def get_batch(img_paths, idx_tup): \n",
    "        return get_imgs(img_paths[idx_tup[0] : idx_tup[1]])\n",
    "    \n",
    "\n",
    "def run_inf(img_paths, processor, model, device=None, batch_size=10):\n",
    "    if device is None: device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    idxs = get_batch_idxs(img_paths)\n",
    "    return list(itertools.chain(*[get_lbls(get_batch(img_paths, idx_tup), processor, model, device) \\\n",
    "                                  for idx_tup in get_batch_idxs(img_paths)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82897f18-da23-4d71-b0ee-d886312be17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/campaign2/0021/image_0021835.png',\n",
       " 'data/campaign2/0021/image_0021801.png',\n",
       " 'data/campaign2/0021/background_0021520.png']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_folder = Path('data/campaign2/0021')\n",
    "img_names = get_file_names(img_folder)\n",
    "img_paths = get_img_paths(img_folder, img_names)\n",
    "img_paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf80fe6-7ed3-420f-bd76-8bb5174eace8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db186f9d9f144b18b1e844fdeca8a2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'Salesforce/blip2-flan-t5-xl'\n",
    "model, processor = get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7ceb8-f94f-4657-aea6-e69650a81db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model = model.vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebccf9c4-9cf2-4ee5-b2a9-02a719d98839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blip2VisionConfig {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"dropout\": 0.0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_size\": 1408,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"initializer_range\": 1e-10,\n",
       "  \"intermediate_size\": 6144,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"model_type\": \"blip_2_vision_model\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 39,\n",
       "  \"patch_size\": 14,\n",
       "  \"projection_dim\": 512,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.27.0.dev0\"\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8dd1b5-d8f0-4f98-8fea-5c25723be83f",
   "metadata": {},
   "source": [
    "Now lets extract a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6cfc9-0d93-48f9-bf4a-2e37c09aec3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((720, 720, 3), torch.Size([3, 720, 720]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np_img = np.array(Image.open(img_paths[0]))\n",
    "tns_img = torch.tensor(np_img.transpose(2, 0,1))\n",
    "np_img.shape, tns_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db270b1-626c-45c3-b708-55151c5652aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 720, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is how we can add the batch dimension\n",
    "tns_img[None,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc364b-ee60-47ae-b850-f327d1c5b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to look inside the model\n",
    "#vision_model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3647410-9c7b-450c-ba3a-88f987ceb0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the first embedding layer (with a conv2d) that reduces the image feature space down\n",
    "#vision_model.embeddings??\n",
    "vision_model.embeddings.patch_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4617cd-d5ef-41ac-8a57-406974b16b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-0.4659, -0.8540,  0.2021,  ..., -0.1645, -0.2268, -0.3574],\n",
       "         [-0.7202, -1.2308, -0.1918,  ...,  0.5010, -0.6158, -0.6872],\n",
       "         [-0.8480, -0.6770, -0.5201,  ..., -0.1174, -0.0808, -1.1502],\n",
       "         ...,\n",
       "         [-0.3844, -2.3725, -0.2407,  ...,  0.1078, -0.9313, -0.1320],\n",
       "         [-0.0489, -2.1740, -0.3484,  ...,  0.1823, -0.5096,  0.0825],\n",
       "         [-0.9160, -1.4308, -0.4508,  ...,  0.8405, -0.6817,  0.6056]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9260, -1.5141,  0.2457,  ..., -0.6644, -0.2725, -0.7160]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = vision_model(tns_img[:,:224, :224][None, ...].float().cuda())\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8aea5-0b69-4a2b-b048-792d654c0385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 257, 1408])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e109d-71a5-4e9c-9db4-6a7661ae71f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
